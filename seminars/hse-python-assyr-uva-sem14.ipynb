{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea17432-ea86-4cb1-bed1-bdd795b70aa0",
   "metadata": {},
   "source": [
    "# Семинар 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8401e33-1c33-4067-a729-22c174c893ca",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784e03a-3359-4122-93db-95072e271ea3",
   "metadata": {},
   "source": [
    "Токенизация — не такая тривиальная задача, как кажется на первый взгляд. Например, как токенизировать выражение «`Hello world!`»? Можно придумать много разных способов:\n",
    "1) `Hello` — `world`\n",
    "2) `Hello` — `world!`\n",
    "3) `Hello` — `world` — `!`\n",
    "4) `Hello` — ` ` — `world` — `!`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414716d0-0856-47bf-a16c-d8f54f321137",
   "metadata": {},
   "source": [
    "Реализуем на питоне регулярное выражение, которое будет токенизировать текст по формату (4) выше, то есть раздельно сохраняя все последовательности букв, знаков препинания и пробелов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d9acd7-9a2a-4b64-a35b-c0e64ba87f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', '!!!', ' ', 'You', ' ', 'are', ' ', 'so', ' ', 'beautiful', ',', ' ', 'isn', \"'\", 't', ' ', 'it', ' ', 'amazing', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello world!!! You are so beautiful, isn't it amazing!\"\n",
    "print(re.findall(\"[A-Za-z]+| +|[\\.,'!\\?]+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88aac4-ae25-4d82-8503-8597eb6da2eb",
   "metadata": {},
   "source": [
    "Получилось! Но, как видите, токенизировать регулярками — не самое лёгкое занятие. Для реального текста нам бы пришлось прописывать ещё миллион отдельных случаев (все знаки препинания, цифры, всякие прочие символы и разные другие проблемы), и регулярное выражение стало бы огромным. Иногда, когда мы хотим настроить в алгоритме токенизации какие-то тонкие детали, у нас просто нет другого выхода. Но для большинства случаев при работе с крупными языками люди пользуются уже готовыми решениями. Одно из них — токенизаторы из библиотеки `nltk`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad267cf-272c-441e-8953-0f5fa668c0b0",
   "metadata": {},
   "source": [
    "### Библиотека `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060208bb",
   "metadata": {},
   "source": [
    "> **Внимание!** На этом семинаре мы будем работать с библиотеками и модулями, не входящими в стандартную библиотеку Python. Чтобы работать с ними на собственном компьютере, их нужно предварительно установить. Если вы пользуетесь питоном в оболочке Anaconda, то это должно быть совсем несложно. Ниже вы найдёте ячейку, которую нужно запустить один раз, чтобы библиотека или модуль установились на ваш компьютер. Если эта ячейка выдала ошибку, нужно поискать решение проблемы. Обратитесь к инструкциям по установке внешних модулей себе на компьютер — их легко можно найти в интернете (вот [пример такой инструкции](https://gb.ru/blog/kak-ustanavlivat-pakety-v-python/)). Пишите, если что-то не получается!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728fbeff-c315-41c5-8b31-1168bf7eb257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Установка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743fba8-8081-4fb5-b0d2-4749fa899ef0",
   "metadata": {},
   "source": [
    "Итак, **`nltk`** — библиотека, то есть такой пакет с пакетами, мега-модуль, содержащий другие модули. Попробуем различные токенизаторы из этой библиотеки (см. [документацию](https://www.nltk.org/index.html) `nltk`). Сначала нужно её импортировать, а ещё докачать специальные файлы, необходимые для работы некоторых инструментов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed308e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk   # <- установка модуля на ваш компьютер (нужно сделать всего один раз)\n",
    "                    # это самый лёгкий способ установить модуль, но для него нужна программа pip\n",
    "                    # (см. инструкцию выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45609081-4455-43d9-b678-2ab7a611cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bdcf076-64cb-4bdc-9e0a-97900c4f9c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Samsung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Samsung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")   # <-- это нужно скачать для токенизации\n",
    "nltk.download(\"stopwords\")   # <-- это нужно скачать для стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df05f41-6c6a-4ae9-93f2-7438faa36415",
   "metadata": {},
   "source": [
    "Будем проверять, как работают разные токенизаторы, на вот этом английском тексте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8560ca-5778-4682-92a1-cb7ddb871120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good muffins cost $3.88  in New York. \n",
      "Please buy me two of them... Or don't!\n",
      "\n",
      "Thanks.\n"
     ]
    }
   ],
   "source": [
    "text = \"Good muffins cost $3.88  in New York. \\nPlease buy me two of them... Or don't!\\n\\nThanks.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f023c8e-033a-432c-9dc6-6b74d1c67272",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Модуль **`nltk.tokenize`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50130a-fcbe-4293-b943-37b1f262ba35",
   "metadata": {},
   "source": [
    "За токенизацию в библиотеке `nltk` отвечает вложенный в неё модуль **`nltk.tokenize`**. Самый простой и часто используемый инструмент для токенизации в нём — функция `word_tokenize()`. Заметьте, что она не просто делит по пробелам, но (1) выделяет знаки препинания, (2) игнорирует пробелы и переход на новую строку, (3) делит стяжённую словоформу *don’t* на *do* и *n’t*, (4) отделяет знак доллара, но сохраняет целиком число с десятичной запятой ($3.88$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06fda5c-864a-420f-859d-f1b8a6541a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '...', 'Or', 'do', \"n't\", '!', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80140d1-99d7-4ade-88a8-f7a29b9cba52",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef241dd-3485-475b-8d34-eacea4964c8f",
   "metadata": {},
   "source": [
    "(На всякий случай вспомним, как мы работаем с модулями. Добраться до функции `word_tokenize()` можно разными способами, вот некоторые из них с пояснениями:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8216c2de-6b63-453e-87cc-976c77f6a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Эй', ',', 'токенизируй', 'меня', ',', 'друг', '!']\n"
     ]
    }
   ],
   "source": [
    "# 1 способ\n",
    "\n",
    "# импортируем всю библиотеку nltk\n",
    "import nltk\n",
    "# используем вложенный в nltk модуль nltk.tokenize, а из него функцию word_tokenize()\n",
    "print(nltk.tokenize.word_tokenize(\"Эй, токенизируй меня, друг!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "177aa385-3971-4737-aa7c-40ee0e2388dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Эй', ',', 'токенизируй', 'меня', ',', 'друг', '!']\n"
     ]
    }
   ],
   "source": [
    "# 2 способ\n",
    "\n",
    "# импортируем только конкретный модуль nltk.tokenize\n",
    "import nltk.tokenize\n",
    "# используем импортированный модуль nltk.tokenize, а из него функцию word_tokenize()\n",
    "print(nltk.tokenize.word_tokenize(\"Эй, токенизируй меня, друг!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d05e58fd-2f3a-4ac2-9bf9-d5247f603640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Эй', ',', 'токенизируй', 'меня', ',', 'друг', '!']\n"
     ]
    }
   ],
   "source": [
    "# 3 способ\n",
    "\n",
    "# импортируем только конкретный модуль nltk.tokenize и называем его каким-нибудь псевдонимом (например, t)\n",
    "import nltk.tokenize as t\n",
    "# обращаемся к модулю nltk.tokenize с помощью псевдонима t и из него используем функцию word_tokenize()\n",
    "print(t.word_tokenize(\"Эй, токенизируй меня, друг!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dde45945-3066-4345-b790-6b1b4aed93d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Эй', ',', 'токенизируй', 'меня', ',', 'друг', '!']\n"
     ]
    }
   ],
   "source": [
    "# 4 способ\n",
    "\n",
    "# импортируем конкретную функцию word_tokenize() из модуля nltk.tokenize\n",
    "# (импортируется только word_tokenize(), другие функции будут недоступны)\n",
    "from nltk.tokenize import word_tokenize\n",
    "# зато использовать функцию word_tokenize() гораздо проще\n",
    "print(word_tokenize(\"Эй, токенизируй меня, друг!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37263374-716b-47b6-a5a3-6fe37ef3c1c2",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36315de-da1e-4127-a4a7-f2964d1ee05d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Другие токенизаторы **`nltk`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1fc575-30d2-480f-95c0-fd14b2e61b44",
   "metadata": {},
   "source": [
    "В `nltk` есть и другие токенизаторы (полный список можно найти в [документации](https://www.nltk.org/index.html)). Посмотрим для примера на ещё один из них, `ToktokTokenizer`. Он используется иначе, чем мы привыкли — это не функция, а уникальный объект, то есть **объект собственного класса**.\n",
    "\n",
    "Классы — это как бы *новые типы данных*, созданные пользователями питона. Как у строк или у списков есть свои методы (`str.split()`, `str.upper()`, `list.append()`), так же и к объектам новых классов можно придумать новые методы. Но только строки — это встроенный тип данных, а `ToktokTokenizer` — это такой особый тип, созданный разработчиками `nltk`. Они прописали, какую информацию объекты этого класса будут хранить, и придумали методы, с помощью которых с этими объектами можно взаимодействовать. Про объект `ToktokTokenizer` можно думать как про специальный инструмент, который будет токенизировать для нас тексты.\n",
    "\n",
    "Сначала импортируем сам класс `ToktokTokenizer`, а потом **создадим новый объект этого класса**. Чтобы создать объект класса, нужно вызвать функцию, которая совпадает с его названием (то есть функцию `ToktokTokenizer()`). Можно сравнить этом с тем, как мы создавали новый (пустой) список функцией `list()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8687f3e-17b3-4b6a-a545-52f93bfd8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560aba46-5bf8-4d0c-a6ec-cbb5b3a2025b",
   "metadata": {},
   "source": [
    "(Обратите внимание, что `ToktokTokenizer` мы импортировали не просто из библиотеки `nltk`, и даже не из её модуля `nltk.tokenize`, а из подмодуля `nltk.tokenize.toktok`, который сам вложен в модуль `nltk.tokenize`. К сожалению, структура конкретных модулей и библиотек может быть довольно запутанной. Но есть и хорошие новости: (1) никто не ожидает, что вы это запомните наизусть, (2) никто и не помнит всё это наизусть. Люди просто гуглят или ищут в документации конкретных библиотек и модулей, как сделать то или иное действие с этими инструментами. Это абсолютно нормально и так делают все!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae592b9-08e3-470b-ba2c-aa763cba918f",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627fc0b-a62f-43bf-b6d7-cd932f907b6a",
   "metadata": {},
   "source": [
    "Итак, теперь у нас есть объект-токенизатор, заточённый в переменной `tokenizer`. Убедимся, что это правда объект отдельного типа `ToktokTokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0073abdb-d7de-4aa3-9edb-9d4312454ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.toktok.ToktokTokenizer object at 0x000001B5C122B810>\n",
      "<class 'nltk.tokenize.toktok.ToktokTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c55fd5-e18b-405b-b2eb-33687b2cebe1",
   "metadata": {},
   "source": [
    "У этого класса есть метод `.tokenize()`, в который можно подать текст. Обратите внимание, что результат слегка отличается от функции `word_tokenize()`, которую мы видели выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bb26f8c-4844-49e0-aafa-7f5a097e077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them', '...', 'Or', 'don', \"'\", 't', '!', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac0705-c35b-41ce-a6d6-2a1b593918e8",
   "metadata": {},
   "source": [
    "(Напоследок: вообще-то записывать объект в переменную необязательно, можно просто создать объект на ходу и тут же вызвать от него метод.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b145adc-61f2-4d75-bf79-ec06f0018dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them', '...', 'Or', 'don', \"'\", 't', '!', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(ToktokTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a210df-94d2-4818-80ce-b1e29469a797",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15bf1cd-5c9c-43c4-96b3-b02faf47c084",
   "metadata": {},
   "source": [
    "В отдельном модуле `nltk.corpus` внутри библиотеки `nltk` можно найти списки **стоп-слов** (*stopwords*) для разных языков. Стоп-слова — это слова (и словоформы), которые частотны в любых текстах на определённом языке, имеют функциональное значение (союзы, предлоги, местоимения, междометия) и поэтому бесполезны при решении задач, в которых нужно понять что-то о смысле текста. Такой список стоп-слов можно собрать вручную, но `nltk` уже любезно сделал это за нас. Чтобы ими воспользоваться, нужно импортировать объект `stopwords` из `nltk.corpus`. У него есть метод `.words()`, в который можно в качестве аргумента подать название языка и получить список со стоп-словами. Такие списки есть для разных языков, включая русский:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d496c4be-9538-47be-8300-2bb5f7417e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87385d23-ed31-4a85-b196-f82c2acc087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7fc01e-0d61-480d-b0fc-5050d4dc7c33",
   "metadata": {},
   "source": [
    "К сожалению, `nltk` подготовил стоп-слова не для всех языков, и, в частности, он не содержит стоп-слова для языков Индокитая (и уж конечно, для древних языков Месопотамии…), но в интернете можно найти кучу подобных списков для разных языков (как в модулях, специализированных для работы с конкретным языком, так и просто в виде списка слов на чьём-нибудь гитхабе)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a49e97a3-6804-49d2-af13-3d61f4e59d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时', '一来', '一样', '一次', '一片', '一直', '一致', '一般', '一起', '一边', '一面', '万一', '上下', '上升', '上去', '上来', '上述', '上面', '下列', '下去', '下来', '下面', '不一', '不久', '不仅', '不会', '不但', '不光', '不单', '不变', '不只', '不可', '不同', '不够', '不如', '不得', '不怕', '不惟', '不成', '不拘', '不敢', '不断', '不是', '不比', '不然', '不特', '不独', '不管', '不能', '不要', '不论', '不足', '不过', '不问', '与', '与其', '与否', '与此同时', '专门', '且', '两者', '严格', '严重', '个', '个人', '个别', '中小', '中间', '丰富', '临', '为', '为主', '为了', '为什么', '为什麽', '为何', '为着', '主张', '主要', '举行', '乃', '乃至', '么', '之', '之一', '之前', '之后', '之後', '之所以', '之类', '乌乎', '乎', '乘', '也', '也好', '也是', '也罢', '了', '了解', '争取', '于', '于是', '于是乎', '云云', '互相', '产生', '人们', '人家', '什么', '什么样', '什麽', '今后', '今天', '今年', '今後', '仍然', '从', '从事', '从而', '他', '他人', '他们', '他的', '代替', '以', '以上', '以下', '以为', '以便', '以免', '以前', '以及', '以后', '以外', '以後', '以来', '以至', '以至于', '以致', '们', '任', '任何', '任凭', '任务', '企图', '伟大', '似乎', '似的', '但', '但是', '何', '何况', '何处', '何时', '作为', '你', '你们', '你的', '使得', '使用', '例如', '依', '依照', '依靠', '促进', '保持', '俺', '俺们', '倘', '倘使', '倘或', '倘然', '倘若', '假使', '假如', '假若', '做到', '像', '允许', '充分', '先后', '先後', '先生', '全部', '全面', '兮', '共同', '关于', '其', '其一', '其中', '其二', '其他', '其余', '其它', '其实', '其次', '具体', '具体地说', '具体说来', '具有', '再者', '再说', '冒', '冲', '决定', '况且', '准备', '几', '几乎', '几时', '凭', '凭借', '出去', '出来', '出现', '分别', '则', '别', '别的', '别说', '到', '前后', '前者', '前进', '前面', '加之', '加以', '加入', '加强', '十分', '即', '即令', '即使', '即便', '即或', '即若', '却不', '原来', '又', '及', '及其', '及时', '及至', '双方', '反之', '反应', '反映', '反过来', '反过来说', '取得', '受到', '变成', '另', '另一方面', '另外', '只是', '只有', '只要', '只限', '叫', '叫做', '召开', '叮咚', '可', '可以', '可是', '可能', '可见', '各', '各个', '各人', '各位', '各地', '各种', '各级', '各自', '合理', '同', '同一', '同时', '同样', '后来', '后面', '向', '向着', '吓', '吗', '否则', '吧', '吧哒', '吱', '呀', '呃', '呕', '呗', '呜', '呜呼', '呢', '周围', '呵', '呸', '呼哧', '咋', '和', '咚', '咦', '咱', '咱们', '咳', '哇', '哈', '哈哈', '哉', '哎', '哎呀', '哎哟', '哗', '哟', '哦', '哩', '哪', '哪个', '哪些', '哪儿', '哪天', '哪年', '哪怕', '哪样', '哪边', '哪里', '哼', '哼唷', '唉', '啊', '啐', '啥', '啦', '啪达', '喂', '喏', '喔唷', '嗡嗡', '嗬', '嗯', '嗳', '嘎', '嘎登', '嘘', '嘛', '嘻', '嘿', '因', '因为', '因此', '因而', '固然', '在', '在下', '地', '坚决', '坚持', '基本', '处理', '复杂', '多', '多少', '多数', '多次', '大力', '大多数', '大大', '大家', '大批', '大约', '大量', '失去', '她', '她们', '她的', '好的', '好象', '如', '如上所述', '如下', '如何', '如其', '如果', '如此', '如若', '存在', '宁', '宁可', '宁愿', '宁肯', '它', '它们', '它们的', '它的', '安全', '完全', '完成', '实现', '实际', '宣布', '容易', '密切', '对', '对于', '对应', '将', '少数', '尔后', '尚且', '尤其', '就', '就是', '就是说', '尽', '尽管', '属于', '岂但', '左右', '巨大', '巩固', '己', '已经', '帮助', '常常', '并', '并不', '并不是', '并且', '并没有', '广大', '广泛', '应当', '应用', '应该', '开外', '开始', '开展', '引起', '强烈', '强调', '归', '当', '当前', '当时', '当然', '当着', '形成', '彻底', '彼', '彼此', '往', '往往', '待', '後来', '後面', '得', '得出', '得到', '心里', '必然', '必要', '必须', '怎', '怎么', '怎么办', '怎么样', '怎样', '怎麽', '总之', '总是', '总的来看', '总的来说', '总的说来', '总结', '总而言之', '恰恰相反', '您', '意思', '愿意', '慢说', '成为', '我', '我们', '我的', '或', '或是', '或者', '战斗', '所', '所以', '所有', '所谓', '打', '扩大', '把', '抑或', '拿', '按', '按照', '换句话说', '换言之', '据', '掌握', '接着', '接著', '故', '故此', '整个', '方便', '方面', '旁人', '无宁', '无法', '无论', '既', '既是', '既然', '时候', '明显', '明确', '是', '是否', '是的', '显然', '显著', '普通', '普遍', '更加', '曾经', '替', '最后', '最大', '最好', '最後', '最近', '最高', '有', '有些', '有关', '有利', '有力', '有所', '有效', '有时', '有点', '有的', '有着', '有著', '望', '朝', '朝着', '本', '本着', '来', '来着', '极了', '构成', '果然', '果真', '某', '某个', '某些', '根据', '根本', '欢迎', '正在', '正如', '正常', '此', '此外', '此时', '此间', '毋宁', '每', '每个', '每天', '每年', '每当', '比', '比如', '比方', '比较', '毫不', '没有', '沿', '沿着', '注意', '深入', '清楚', '满足', '漫说', '焉', '然则', '然后', '然後', '然而', '照', '照着', '特别是', '特殊', '特点', '现代', '现在', '甚么', '甚而', '甚至', '用', '由', '由于', '由此可见', '的', '的话', '目前', '直到', '直接', '相似', '相信', '相反', '相同', '相对', '相对而言', '相应', '相当', '相等', '省得', '看出', '看到', '看来', '看看', '看见', '真是', '真正', '着', '着呢', '矣', '知道', '确定', '离', '积极', '移动', '突出', '突然', '立即', '第', '等', '等等', '管', '紧接着', '纵', '纵令', '纵使', '纵然', '练习', '组成', '经', '经常', '经过', '结合', '结果', '给', '绝对', '继续', '继而', '维持', '综上所述', '罢了', '考虑', '者', '而', '而且', '而况', '而外', '而已', '而是', '而言', '联系', '能', '能否', '能够', '腾', '自', '自个儿', '自从', '自各儿', '自家', '自己', '自身', '至', '至于', '良好', '若', '若是', '若非', '范围', '莫若', '获得', '虽', '虽则', '虽然', '虽说', '行为', '行动', '表明', '表示', '被', '要', '要不', '要不是', '要不然', '要么', '要是', '要求', '规定', '觉得', '认为', '认真', '认识', '让', '许多', '论', '设使', '设若', '该', '说明', '诸位', '谁', '谁知', '赶', '起', '起来', '起见', '趁', '趁着', '越是', '跟', '转动', '转变', '转贴', '较', '较之', '边', '达到', '迅速', '过', '过去', '过来', '运用', '还是', '还有', '这', '这个', '这么', '这么些', '这么样', '这么点儿', '这些', '这会儿', '这儿', '这就是说', '这时', '这样', '这点', '这种', '这边', '这里', '这麽', '进入', '进步', '进而', '进行', '连', '连同', '适应', '适当', '适用', '逐步', '逐渐', '通常', '通过', '造成', '遇到', '遭到', '避免', '那', '那个', '那么', '那么些', '那么样', '那些', '那会儿', '那儿', '那时', '那样', '那边', '那里', '那麽', '部分', '鄙人', '采取', '里面', '重大', '重新', '重要', '鉴于', '问题', '防止', '阿', '附近', '限制', '除', '除了', '除此之外', '除非', '随', '随着', '随著', '集中', '需要', '非但', '非常', '非徒', '靠', '顺', '顺着', '首先', '高兴', '是不是']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"chinese\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ccc32-b05d-4bd8-a6a7-8d6173776f18",
   "metadata": {},
   "source": [
    "**Удаление стоп-слов** — стандартная процедура при предобработке текста для очень многих задач NLP. Например, когда вы токенизировали текст, можно удалить все токены, которые встречаются среди стоп-слов (в цикле поставив условие наподобие `if token in stopwords.words(\"russian\")`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13bc93-ff75-45d2-a0c0-0d55ace5781a",
   "metadata": {},
   "source": [
    "## Автоматический морфологический анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9d7a0-9f5e-4477-a939-fa2eb00c19b4",
   "metadata": {},
   "source": [
    "Что мы хотим <s>от этой жизни</s> от автоматического морфологического анализа?\n",
    "\n",
    "Вот наша троица мечты:\n",
    "1. **POS-тэггинг** (*part-of-speech tagging*, частеречный тэггинг) — чтобы компьютер сам определял часть речи и проставлял соответствующие тэги всем токенам\n",
    "2. **определение грамматических свойств** — чтобы компьютер определял для каждого токена грамматические времена, наклонения, род, число, падеж, изафет и всё прочее\n",
    "3. **лемматизация** — чтобы компьютер определял **лемму**, то есть, по-школьному, начальную форму для каждого токена\n",
    "\n",
    "Сейчас посмотрим, как это всё можно делать на примере русского языка."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4284c80-15d9-4f51-b11c-c4dceccba20b",
   "metadata": {},
   "source": [
    "### Русский язык"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67e678-22d4-4818-a4a9-e6c38a35cd5e",
   "metadata": {},
   "source": [
    "Вот некоторые библиотеки для работы с русским языком:\n",
    "- [**`pymorphy3`**](https://github.com/no-plagiarism/pymorphy3) — компактный и простой в использовании модуль, не самый точный и быстрый (но тоже очень хороший)\n",
    "- [`natasha`](https://natasha.github.io) — большая и мощная библиотека с кучей инструментов для русского\n",
    "- [`spacy`](https://spacy.io) — межъязыковая библиотека с моделями, натренированными решать разные NLP-задачи для разных языков, [включая русский](https://habr.com/ru/articles/531940/)\n",
    "\n",
    "Мы изучим вопрос морфологического анализа на примере **`pymorphy3`**. С его документацией можно ознакомиться **[вот здесь](https://pymorphy2.readthedocs.io/en/stable/user/guide.html#id3)** (рекомендую, это очень приятная и понятная документация!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f1ff0-a1ee-4d48-b263-5791bb304d09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Установка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd3f1e-5dfe-4eab-af43-bcd4a6f8e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymorphy3   # <- установка модуля на ваш компьютер (нужно сделать всего один раз)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e50e266-9806-4634-ba6b-2668543bbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825f0a8-70f6-4358-8646-39d23f8b65e6",
   "metadata": {},
   "source": [
    "Модуль `pymorphy3` тоже построен на *объектах*, а не на *функциях*. Самый важный инструмент в нём — это класс `MorphAnalyzer`. Это такая машинка, которая занимается морфоанализом. Инициализируем объект этого класса (вызвав `MorphAnalyzer()`) и запишем этот объект в переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ce2dec2-062d-4e6b-b4fc-c6157e6179cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = pymorphy3.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca0171-41a2-4e2e-b651-cc809ef738a8",
   "metadata": {},
   "source": [
    "Обратите внимание, что при создании объекта этого класса компьютер может немножечко подвисать. Это потому, что питон подгружает разные механизмы, необходимые для морфологического анализа, из файлов модуля `pymorphy3` в оперативную память вашего компьютера. Было бы нецелесообразно замусоривать оперативную память, поэтому общепринятая практика — создавать такие объекты один раз за код, сохранять их в отдельную переменную (как мы и сделали) и затем пользоваться этой переменной (а не создавать новый объект каждый раз, когда нужно что-то разобрать)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2f189-f7ad-4a4d-b489-2a4a4efbd5e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Метод **`.parse()`** и класс **`Parse`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393bf48-2c3b-4dc3-a07a-a8a8865826a1",
   "metadata": {},
   "source": [
    "Основной метод класса `MorphAnalyzer` называется **`.parse()`**. В него нужно подавать токены на русском языке (по отдельности!), и тогда наш ручной морфоанализатор будет их парсить, то есть разбирать. Посмотрим, что получится, если подать слово «стекло»:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b881502d-3927-4a58-9580-8ac70aa80d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.690476, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),)),\n",
       " Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 3),)),\n",
       " Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.023809, methods_stack=((DictionaryAnalyzer(), 'стекло', 1015, 3),))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = analyzer.parse(\"стекло\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5eb54-8392-4900-8ba3-b60e16a16583",
   "metadata": {},
   "source": [
    "Из метода `.parse()` мы получили список из объектов нового для нас класса, класса **`Parse`**, то бишь «анализ». Такие объекты содержат результат разбора. Например, первый объект в получившемся списке сообщает следующее:\n",
    "\n",
    "- токен, который в меня подали: `стекло`\n",
    "- тэги, которые я могу предложить для этого токена: существительное (`NOUN`), неодушевлённое (`inan`), среднего рода единственного числа (`neut sing`), в именительном падеже (`nomn`)\n",
    "- лемма («нормальная» форма) этой лексемы: `стекло`\n",
    "\n",
    "Однако в списке больше одного «разбора», потому что слово «стекло» неоднозначно. Оно может значить существительное «стекло» в именительном падеже (это первый разбор, с индексом 0), а может существительное «стекло» в винительном падеже (разбор с индексом 1), а ещё может форму прошедшего времени глагола «стечь» (разбор с индексом 2). Число `score` в получившихся разборах отражает вероятность, что данный анализ является верным. Как видите, наш анализатор посчитал, что «стекло» в именительном падеже — наиболее вероятный сценарий, и присвоил ему вероятность в $69$%, а такая форма глагола «стечь» встречается гораздо реже, поэтому её вероятность всего $2$%. Разборы в списке упорядочиваются по убыванию вероятности, так что, если нужен наиболее вероятный разбор из возможных, можно всегда брать первый."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f57fa-2358-46ce-9b05-e90212f62ccc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Атрибуты **`Parse`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db53089-588a-4205-9995-e97af6c7540d",
   "metadata": {},
   "source": [
    "У объекта типа `Parse` есть несколько атрибутов: тэги, лемма, вероятность и прочее. Термином «**атрибуты**» в питоне обозначаются какие-то свойства объекта, которые из него можно достать. Обращаться ним надо так же, как к методам класса, только с методами мы (как с функциями) пишем круглые скобки, а с атрибутами нет. (**[Вот здесь](https://stackoverflow.com/questions/46312470/difference-between-methods-and-attributes-in-python)** отличное объяснение разницы между атрибутами и функциями, очень рекомендую.)\n",
    "\n",
    "Например, проанализируем слово «ласковых», возьмём его первый (самый вероятный) разбор и достанем из него лемму с помощью атрибута **`.normal_form`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e52c6ba-985e-4cc9-96f8-48b175e5bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='ласковых', tag=OpencorporaTag('ADJF,Qual plur,gent'), normal_form='ласковый', score=0.5, methods_stack=((DictionaryAnalyzer(), 'ласковых', 249, 21),))\n",
      "Лемма: ласковый\n"
     ]
    }
   ],
   "source": [
    "result = analyzer.parse(\"ласковых\")[0]   # не глядя берём разбор с индексом 0\n",
    "print(result)  # выведем весь разбор\n",
    "\n",
    "print(\"Лемма:\", result.normal_form)  # выведем только лемму"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd14fa-31d2-44c3-b718-8848e841df9c",
   "metadata": {},
   "source": [
    "А вот ещё один атрибут, **`.tag`**, список тэгов-грамматических свойств токена. Здесь уже чуть более непонятные тэги: `ADJF` — прилагательное в полной форме (*ADJective*+*Full*), `Qual` — качественное прилагательное (*Qualitative*), `plur` — множественное число, `gent` — родительный падеж (генитив).\n",
    "\n",
    "Первый тэг, записанный прописными буквами, — всегда часть речи, остальные — прочие свойства. В `pymorphy3` используются тэги из проекта [OpenCorpora](https://www.opencorpora.org), (почти) полный список этих тэгов можно найти **[здесь](https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html#grammeme-docs)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6b3f811-ef46-47a3-a621-99fc46a0b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJF,Qual plur,gent\n"
     ]
    }
   ],
   "source": [
    "tag = result.tag\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486160d-caf0-492b-8431-02394e2ec1ce",
   "metadata": {},
   "source": [
    "Может показаться, что атрибут `.tag` выдаёт просто обычную строку. Результат действительно похож на строку по многим свойствам: например, с помощью `in` можно проверить, является ли этот токен словом определённой части речи, стоит ли он в определённом падеже, времени и так далее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c07f6eb-908f-446a-975e-fcbbe6757877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"gent\" in tag)\n",
    "print(\"ADJF\" in tag)\n",
    "\n",
    "print(\"nomn\" in tag)\n",
    "print(\"NOUN\" in tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b5da6-7e04-4212-9a0e-2ff9a3345b49",
   "metadata": {},
   "source": [
    "Но на самом деле это не просто строка, а… объект ещё одного отдельного класса. (Да, так устроены все большие модули в питоне, и это норма!) Убедимся в этом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "620ad606-bb01-4d46-ac13-1c1c9ff06bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pymorphy3.tagset.OpencorporaTag'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61be6f1-3c53-438b-b6be-29dfa44e3eff",
   "metadata": {},
   "source": [
    "В том, что это не просто строка, есть несколько плюсов. Во-первых, если спросить, нет ли в этом разборе какого-то тэга, который отсутствует в наборе тэгов OpenCorpora, `pymorphy3` это заметит и скажет вам. Допустим, вы хотели найти в тексте все слова в родительном падеже, но забыли тэг для него и написали что-то не то. Модуль не выдаст ответа и вместо этого укажет на ошибку — мол, граммемы такой не знаю, ты ошибся:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18151f03-7d3e-4ba8-a31e-c79f6960ed3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammeme is unknown: geni",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeni\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymorphy3\\tagset.py:341\u001b[39m, in \u001b[36mOpencorporaTag.__contains__\u001b[39m\u001b[34m(self, grammeme)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grammeme_is_known(grammeme):\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGrammeme is unknown: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrammeme\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Grammeme is unknown: geni"
     ]
    }
   ],
   "source": [
    "print(\"geni\" in tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c4156-6f98-4d39-be02-126fc474a104",
   "metadata": {},
   "source": [
    "Во-вторых, если нужно найти в цепочке тэгов сразу несколько тэгов (например, проверить токен на число и падеж одновременно), это можно сделать с помощью множества (помните такой тип данных?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a78ba56-b5b4-4e22-a49f-37223fbe51bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"gent\" in tag)\n",
    "print(\"plur\" in tag)\n",
    "\n",
    "print({\"gent\",\"plur\"} in tag)   # верно ли, что слово и в генитиве, и в мн. ч.? — верно\n",
    "print({\"gent\",\"sing\"} in tag)   # верно ли, что слово и в генитиве, и в ед. ч.? — неверно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe2c65-8f52-43a1-885b-21be64b7e15f",
   "metadata": {},
   "source": [
    "В-третьих, у этого объекта у самого есть атрибуты — с их помощью можно достать отдельные тэги, такие как часть речи, падеж, число и так далее (и вот это уже обычные строки!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f6afd27-2b5d-437b-ac35-1135d2996fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJF\n",
      "plur\n",
      "gent\n"
     ]
    }
   ],
   "source": [
    "print(tag.POS)\n",
    "print(tag.number)\n",
    "print(tag.case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443a6e5-58d8-47c3-83b8-e8c0107c8910",
   "metadata": {},
   "source": [
    "Заметьте, что если мы разобрали существительное, то, например, признака «грамматическое время» у него, конечно, не будет. Но атрибут такой всё равно есть, просто выдаёт он объект `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71d83b38-4c03-46e0-95c2-e603e98f23d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tag.tense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c911c2-19f4-4aef-92ad-b07f300f39e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Практические задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb897131",
   "metadata": {},
   "source": [
    "Дан случайный русский текст (из Национального корпуса русского языка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de4fd82d-a259-46c2-84c7-6926d979bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Довольно любопытно, неужели только меня возмущают смещенные акценты \n",
    "в угоду малозначительной мишуры? Где затерялся тот первозданный минимализм \n",
    "и первоклассная стабильность, оптимизация? Уже молчу о подгрузке контента \n",
    "и качестве звонков ― это нечто лежащее за пределами моего понимания. \n",
    "И как бы то прискорбно не прозвучало, но вынужден констатировать: \n",
    "текущие метаморфозы телеги ― это первосортный кринж в чистом виде. \n",
    "По крайней мере, на мой скромный взгляд через ретроспективу. \n",
    "Ощущение, будто в одноклассники заглянул, бррррр… Сам бы не поверил \n",
    "собственным словам пару-тройку лет назад, но даже whatsapp выглядит \n",
    "более выигрышно в этой ситуации. А вообще, пожалуй, начну \n",
    "планомерно мигрировать в сигнал.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf14b9-7ea1-47ea-98ca-2f90107af800",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199ee3c",
   "metadata": {},
   "source": [
    "#### Задача 14.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06907d31-45ea-41f8-b228-369156e71d0f",
   "metadata": {},
   "source": [
    "Токенизируйте этот текст с помощью функции `word_tokenize()` из модуля `nltk.tokenize`. Удалите стоп-слова с помощью списка из модуля `nltk.corpus`. Результат сохраните в список и выведите этот список на экран в любом удобном вам формате. (Обязательно пропишите корректный импорт для модуля / функции, чтобы ячейку с вашим решением можно было запускать отдельно от остальной тетрадки.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57401164-ec7e-43d1-8326-ed145763c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваше решение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e027d39",
   "metadata": {},
   "source": [
    "#### Задача 14.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c6004-23c6-412d-b199-6c577d513323",
   "metadata": {},
   "source": [
    "Импортируйте и создайте новый объект класса `MorphAnalyzer` из `pymorphy3` (чтобы эту ячейку можно было запустить автономно, как и предыдущую). Используя этот анализатор и полученный в предыдущем задании список токенов, найдите в тексте все наречия (используйте только наиболее вероятный разбор из предлагаемых). Выведите их на экран в любом удобном вам формате.\n",
    "\n",
    "*Подсказка: у вас должно получиться шесть наречий.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df7ef6-de11-401f-bd1f-329a6faaba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваше решение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2eea2",
   "metadata": {},
   "source": [
    "#### Задача 14.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb0a9a-b512-400e-8794-224ef0417052",
   "metadata": {},
   "source": [
    "Используя уже созданный в предыдущей задаче объект-анализатор, найдите в тексте все имена существительные в единственном числе и родительном падеже. Для каждого слова выведите на экран через пробел (1) лемму слова и (2) его тэги (например, `понимание NOUN,inan,neut sing,gent`).\n",
    "\n",
    "*Подсказка: у вас должно получиться четыре таких слова.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0cc6f-4009-4fd9-aeb1-6a43f4d26623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваше решение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01392b81",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30887bf9-e068-4b79-b8b6-dfc23325330a",
   "metadata": {},
   "source": [
    "### Модули для работы с языками Месопотамии и Юго-Восточной Азии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3ed1d-4659-4614-ac19-c1e8e48a6f7e",
   "metadata": {},
   "source": [
    "Вот некоторые модули, которые могут пригодиться вам в работе с вашими языками:\n",
    "- [**`VnCoreNLP`**](https://github.com/vncorenlp/VnCoreNLP) — модуль для вьетнамского языка (токенизация, POS-тэггинг, синтаксический анализ, извлечение именованных сущностей)\n",
    "- [**`underthesea`**](https://github.com/undertheseanlp/underthesea) — модуль для вьетнамского языка (токенизация, нормализация, POS-тэггинг, синтаксический анализ, извлечение именованных сущностей, классификация текста, машинный перевод, _text-to-speech_)\n",
    "- [**`Akkademia`**](https://github.com/gaigutherz/Akkademia) — модуль для транслитерации клинописных текстов\n",
    "    - см. также страницу [*Computational Assyriology*](https://oracc.museum.upenn.edu/compass/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
